{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x, y, b=1):\n",
    "    \"\"\"Returns the linear combination of arrays `x` and `y` with\n",
    "    the optional bias term `b` (set to 1 by default).\"\"\"\n",
    "    \n",
    "    return np.dot(x,y.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to optimize (OR OPTIMIZER)\n",
    "\n",
    "def objective_function(alphas, target, kernel, X_train):\n",
    "    \"\"\"Returns the SVM objective function based in the input model defined by:\n",
    "    `alphas`: vector of Lagrange multipliers\n",
    "    `target`: vector of class labels (-1 or 1) for training data\n",
    "    `kernel`: kernel function\n",
    "    `X_train`: training data for model.\"\"\"\n",
    "    \n",
    "    return np.sum(alphas) - 0.5 * np.sum(target * target * kernel(X_train, X_train) * alphas * alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step(i1, i2, model):\n",
    "    \n",
    "    # Skip if chosen alphas are the same\n",
    "    if i1 == i2:\n",
    "        return 0, model\n",
    "    \n",
    "    alph1 = model.alphas[i1]\n",
    "    alph2 = model.alphas[i2]\n",
    "    y1 = model.y[i1]\n",
    "    y2 = model.y[i2]\n",
    "    E1 = model.errors[i1]\n",
    "    E2 = model.errors[i2]\n",
    "    s = y1 * y2\n",
    "    \n",
    "    # Compute L & H, the bounds on new possible alpha values\n",
    "    if (y1 != y2):\n",
    "        L = max(0, alph2 - alph1)\n",
    "        H = min(model.C, model.C + alph2 - alph1)\n",
    "    elif (y1 == y2):\n",
    "        L = max(0, alph1 + alph2 - model.C)\n",
    "        H = min(model.C, alph1 + alph2)\n",
    "    if (L == H):\n",
    "        return 0, model\n",
    "\n",
    "    # Compute kernel & 2nd derivative eta\n",
    "    k11 = model.kernel(model.X[i1], model.X[i1])\n",
    "    k12 = model.kernel(model.X[i1], model.X[i2])\n",
    "    k22 = model.kernel(model.X[i2], model.X[i2])\n",
    "    eta = 2 * k12 - k11 - k22\n",
    "    \n",
    "    # Compute new alpha 2 (a2) if eta is negative\n",
    "    if (eta < 0):\n",
    "        a2 = alph2 - y2 * (E1 - E2) / eta\n",
    "        # Clip a2 based on bounds L & H\n",
    "        if L < a2 < H:\n",
    "            a2 = a2\n",
    "        elif (a2 <= L):\n",
    "            a2 = L\n",
    "        elif (a2 >= H):\n",
    "            a2 = H\n",
    "            \n",
    "    # If eta is non-negative, move new a2 to bound with greater objective function value\n",
    "    else:\n",
    "        alphas_adj = model.alphas.copy()\n",
    "        alphas_adj[i2] = L\n",
    "        # objective function output with a2 = L\n",
    "        Lobj = objective_function(alphas_adj, model.y, model.kernel, model.X) \n",
    "        alphas_adj[i2] = H\n",
    "        # objective function output with a2 = H\n",
    "        Hobj = objective_function(alphas_adj, model.y, model.kernel, model.X)\n",
    "        if Lobj > (Hobj + eps):\n",
    "            a2 = L\n",
    "        elif Lobj < (Hobj - eps):\n",
    "            a2 = H\n",
    "        else:\n",
    "            a2 = alph2\n",
    "            \n",
    "    # Push a2 to 0 or C if very close\n",
    "    if a2 < 1e-8:\n",
    "        a2 = 0.0\n",
    "    elif a2 > (model.C - 1e-8):\n",
    "        a2 = model.C\n",
    "    \n",
    "    # If examples can't be optimized within epsilon (eps), skip this pair\n",
    "    if (np.abs(a2 - alph2) < eps * (a2 + alph2 + eps)):\n",
    "        return 0, model\n",
    "    \n",
    "    # Calculate new alpha 1 (a1)\n",
    "    a1 = alph1 + s * (alph2 - a2)\n",
    "    \n",
    "    # Update threshold b to reflect newly calculated alphas\n",
    "    # Calculate both possible thresholds\n",
    "    b1 = E1 + y1 * (a1 - alph1) * k11 + y2 * (a2 - alph2) * k12 + model.b\n",
    "    b2 = E2 + y1 * (a1 - alph1) * k12 + y2 * (a2 - alph2) * k22 + model.b\n",
    "    \n",
    "    # Set new threshold based on if a1 or a2 is bound by L and/or H\n",
    "    if 0 < a1 and a1 < C:\n",
    "        b_new = b1\n",
    "    elif 0 < a2 and a2 < C:\n",
    "        b_new = b2\n",
    "    # Average thresholds if both are bound\n",
    "    else:\n",
    "        b_new = (b1 + b2) * 0.5\n",
    "\n",
    "    # Update model object with new alphas & threshold\n",
    "    model.alphas[i1] = a1\n",
    "    model.alphas[i2] = a2\n",
    "    \n",
    "    # Update error cache\n",
    "    # Error cache for optimized alphas is set to 0 if they're unbound\n",
    "    for index, alph in zip([i1, i2], [a1, a2]):\n",
    "        if 0.0 < alph < model.C:\n",
    "            model.errors[index] = 0.0\n",
    "    \n",
    "    # Set non-optimized errors based on equation 12.11 in Platt's book\n",
    "    non_opt = [n for n in range(model.m) if (n != i1 and n != i2)]\n",
    "    model.errors[non_opt] = model.errors[non_opt] +\\\n",
    "                            y1*(a1 - alph1)*model.kernel(model.X[i1], model.X[non_opt]) +\\\n",
    "                            y2*(a2 - alph2)*model.kernel(model.X[i2], model.X[non_opt]) + model.b - b_new\n",
    "    \n",
    "    # Update model threshold\n",
    "    model.b = b_new\n",
    "    \n",
    "    return 1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_example(i2, model):\n",
    "    \n",
    "    y2 = model.y[i2]\n",
    "    alph2 = model.alphas[i2]\n",
    "    E2 = model.errors[i2]\n",
    "    r2 = E2 * y2\n",
    "\n",
    "    # Proceed if error is within specified tolerance (tol)\n",
    "    if ((r2 < -tol and alph2 < model.C) or (r2 > tol and alph2 > 0)):\n",
    "        \n",
    "        if len(model.alphas[(model.alphas != 0) & (model.alphas != model.C)]) > 1:\n",
    "            # Use 2nd choice heuristic is choose max difference in error\n",
    "            if model.errors[i2] > 0:\n",
    "                i1 = np.argmin(model.errors)\n",
    "            elif model.errors[i2] <= 0:\n",
    "                i1 = np.argmax(model.errors)\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "            \n",
    "        # Loop through non-zero and non-C alphas, starting at a random point\n",
    "        for i1 in np.roll(np.where((model.alphas != 0) & (model.alphas != model.C))[0],\n",
    "                          np.random.choice(np.arange(model.m))):\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "        \n",
    "        # loop through all alphas, starting at a random point\n",
    "        for i1 in np.roll(np.arange(model.m), np.random.choice(np.arange(model.m))):\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "    \n",
    "    return 0, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOModel:\n",
    "    \"\"\"Container object for the model used for sequential minimal optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, C, kernel, alphas, b, errors):\n",
    "        self.X = X               # training data vector\n",
    "        self.y = y               # class label vector\n",
    "        self.C = C               # regularization parameter\n",
    "        self.kernel = kernel     # kernel function\n",
    "        self.alphas = alphas     # lagrange multiplier vector\n",
    "        self.b = b               # scalar bias term\n",
    "        self.errors = errors     # error cache\n",
    "        self._obj = []           # record of objective function value\n",
    "        self.m = len(self.X)     # store size of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    numChanged = 0\n",
    "    examineAll = 1\n",
    "    iterations = 0\n",
    "    \n",
    "    while(numChanged > 0) or (examineAll):\n",
    "        iterations += 1\n",
    "#         print(iterations)\n",
    "        if(iterations>5000):\n",
    "            break\n",
    "        numChanged = 0\n",
    "        if examineAll:\n",
    "            # loop over all training examples\n",
    "            for i in range(model.alphas.shape[0]):\n",
    "                examine_result, model = examine_example(i, model)\n",
    "                numChanged += examine_result\n",
    "                if examine_result:\n",
    "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
    "                    model._obj.append(obj_result)\n",
    "        else:\n",
    "            # loop over examples where alphas are not already at their limits\n",
    "            for i in np.where((model.alphas != 0) & (model.alphas != model.C))[0]:\n",
    "                examine_result, model = examine_example(i, model)\n",
    "                numChanged += examine_result\n",
    "                if examine_result:\n",
    "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
    "                    model._obj.append(obj_result)\n",
    "        if examineAll == 1:\n",
    "            examineAll = 0\n",
    "        elif numChanged == 0:\n",
    "            examineAll = 1\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives predictions\n",
    "def predict_decision(alphas, target, kernel, X_train, x_test, b):\n",
    "    \"\"\"Applies the SVM decision function to the input feature vectors in `x_test`.\"\"\"\n",
    "    \n",
    "    result = np.dot((alphas*target),kernel(X_train, x_test))-b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVM For Binary(2 class) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters and initial values\n",
    "C = 1.0\n",
    "#m = len(X_train)\n",
    "#initial_alphas = np.zeros(m)\n",
    "initial_b = 0.0\n",
    "\n",
    "# Set tolerances\n",
    "tol = 0.01 # error tolerance\n",
    "eps = 0.01 # alpha tolerance\n",
    "\n",
    "# Instantiate model\n",
    "#model = SMOModel(X_train, y_train, C, linear_kernel,initial_alphas, initial_b, np.zeros(m))\n",
    "#output = train(model)\n",
    "\n",
    "# For prediction\n",
    "#predictions = decision_function(model.alphas, model.y, model.kernel,model.X, X_test, model.b)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Multiclass Classification Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-white.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,df.columns!='quality'].values\n",
    "Y=df.iloc[:,df.columns=='quality'].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#normalizing\n",
    "normalized_X= preprocessing.normalize(X)\n",
    "standardized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(standardized_X,Y,test_size=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3918, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Out One Vs Rest (OVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TRAINING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)  ## purai array ma unique values haru matrai select garne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters and initial values\n",
    "C = 1.0\n",
    "m = len(x_train)\n",
    "initial_alphas = np.zeros(m)\n",
    "initial_b = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 3  vs Rest\n",
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "Classifier 4  vs Rest\n",
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "Classifier 5  vs Rest\n",
      "[-1  1 -1 ...  1 -1  1]\n",
      "Classifier 6  vs Rest\n",
      "[-1 -1  1 ... -1  1 -1]\n",
      "Classifier 7  vs Rest\n",
      "[ 1 -1 -1 ... -1 -1 -1]\n",
      "Classifier 8  vs Rest\n",
      "[-1 -1 -1 ... -1 -1 -1]\n",
      "Classifier 9  vs Rest\n",
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(y_train)\n",
    "classifiers=[]\n",
    "# iterate over all labels ..\n",
    "for label in labels:\n",
    "    print (\"Classifier\",label,\" vs Rest\")\n",
    "    yi = np.array(y_train)\n",
    "    yi[yi != label] = -1.0\n",
    "    yi[yi == label] = 1.0\n",
    "    print(yi)\n",
    "    model = SMOModel(x_train, y_train, C, linear_kernel,initial_alphas, initial_b, np.zeros(m))\n",
    "    train(model)\n",
    "    classifiers.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of classifier created  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of classifier created \",len(classifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 7)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have to return prediction for x_test array of 11 features Dimension[x,11]\n",
    "n = x_test.shape[0]\n",
    "scores = np.zeros((n, len(labels)))\n",
    "scores.shape #each row of scores will store score(probability) for each label(class) and we will return largest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    model = classifiers[i]\n",
    "    scores[:,i] = predict_decision(model.alphas, model.y, model.kernel,model.X, x_test, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.argmax(scores, axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
